# README ‚Äî Analisador L√©xico (Lexer)

Arquivo: `src/lsi_lexer.py`

O `lsi_lexer.py` √© respons√°vel pela primeira fase da compila√ß√£o: a an√°lise l√©xica. Ele transforma a *string* de c√≥digo-fonte em uma sequ√™ncia de **tokens** reconhec√≠veis pela gram√°tica da linguagem LSI.

***

## ‚öôÔ∏è Defini√ß√£o e Categorias de Tokens

O Lexer reconhece a seguinte lista fixa de tokens, que s√£o categorizados internamente para simplificar o processo de `maximal munch`:

| Categoria | Tokens (Lexemas) | Tipo de Token |
| :--- | :--- | :--- |
| **Palavras-Chave** (`KEYWORDS`) | `int`, `if`, `else`, `def`, `print`, `return` | DEF, INT, IF, ELSE, PRINT, RETURN |
| **Operadores Multi-Caractere** | `<=`, `>=`, `==`, `!=` | LE, GE, EQ, NE |
| **Operadores Simples** | `<`, `>` | LT, GT |
| **Tokens de Caractere √önico** | `+`, `-`, `*`, `/`, `=`, `(`, `)`, `{`, `}`, `,`, `;` | PLUS, MINUS, TIMES, DIV, EQUAL, LPAREN, RPAREN, LBRACE, RBRACE, COMMA, SEMI |
| **Literais** | Qualquer sequ√™ncia de d√≠gitos. | NUM |
| **Identificadores** | Qualquer sequ√™ncia que comece com letra ou `_`. | ID |
| **Fim de Arquivo** | `$` | EOF |

Todos os tokens s√£o representados pela classe `Token` (`typ`, `lexeme`, `line`, `col`).

***

## üß† Mecanismo de An√°lise (FSM e Maximal Munch)

O analisador opera como uma **M√°quina de Estados Finitos (FSM)** impl√≠cita, lendo a entrada **caractere por caractere** (`peek` e `advance`) e seguindo uma ordem estrita de preced√™ncia para tokenizar:

1.  **Pr√©-Processamento:** O m√©todo `skip_space_and_comments` remove:
    * **Espa√ßos em branco:** Quebras de linha, tabula√ß√µes, etc.
    * **Coment√°rios de Linha √önica:** O Lexer ignora qualquer texto que comece com `//` at√© o final da linha.

2.  **Maximal Munch:** O Lexer adere √† regra de **`maximal munch`**, priorizando a captura do token mais longo poss√≠vel. Isso √© crucial para operadores:
    * **Prioridade 1 (Operadores Multi-Caractere):** Ele tenta identificar operadores de dois caracteres (`<=`, `!=`, etc.) antes de tratar seus componentes como tokens de caractere √∫nico. Por exemplo, `i <= j` √© tokenizado como `LE`, e n√£o `LT` seguido de `EQUAL`.
    * **Prioridade 2 (Identificadores e Palavras-Chave):** Uma vez que uma sequ√™ncia de caracteres alfanum√©ricos √© lida, o Lexer verifica se ela √© uma das palavras-chave reservadas (`KEYWORDS`). Caso contr√°rio, √© classificada como um **ID**.

3.  **N√∫meros e Outros Tokens:** Sequ√™ncias de d√≠gitos s√£o lidas como **NUM**s. Por fim, os operadores de caractere √∫nico e a pontua√ß√£o simples s√£o tratados.

***

## üóÉÔ∏è Tabela de S√≠mbolos

O Lexer mant√©m uma **Tabela de S√≠mbolos** (`self.symbol_table`), que √© um dicion√°rio que mapeia o lexema para um objeto de metadados.

* **Pr√©-Carregamento:** A tabela √© inicializada com todas as **Palavras-Chave** da linguagem, marcando seu `kind` como `"keyword"`.
* **Identificadores (`ID`):** Sempre que um novo identificador √© encontrado, ele √© automaticamente inserido na tabela com `kind: "id"`.
* **Prop√≥sito:** Embora o Lexer use a tabela apenas para distinguir Identificadores de Palavras-Chave, essa estrutura √© essencial para as fases subsequentes de An√°lise Sem√¢ntica, onde informa√ß√µes adicionais (tipo, escopo, etc.) ser√£o anexadas.

***

## üõë Sa√≠da e Tratamento de Erros

* **Sa√≠da:** O m√©todo principal `tokenize_all` retorna uma tupla contendo a **lista completa de tokens** (incluindo o token `EOF:$` no final) e a **tabela de s√≠mbolos** finalizada.
* **Erros:** O Lexer √© robusto apenas para o erro de **caractere inv√°lido**.
    * Em caso de um caractere que n√£o inicia nenhum token v√°lido (ex: `@`, `!`), uma exce√ß√£o `LexerError` √© lan√ßada.
    * A mensagem de erro √© formatada para incluir a localiza√ß√£o exata: `Erro l√©xico em linha: L Coluna: C ‚Äî caractere inv√°lido 'x'`.
    * O fluxo de execu√ß√£o √© encerrado imediatamente.
* **Execu√ß√£o:** O Lexer √© instanciado e utilizado pelo `src/lsi_parser.py` para fornecer o fluxo de tokens para o Analisador Sint√°tico.